{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation in Machine Learning\n",
    "\n",
    "This notebook demonstrates various cross-validation techniques used in machine learning to evaluate model performance and prevent overfitting.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Cross-Validation](#introduction)\n",
    "2. [Sample Data Preparation](#data)\n",
    "3. [K-Fold Cross-Validation](#kfold)\n",
    "4. [Stratified K-Fold Cross-Validation](#stratified)\n",
    "5. [Leave-One-Out Cross-Validation (LOOCV)](#loocv)\n",
    "6. [Time Series Split](#timeseries)\n",
    "7. [Cross-Validation Scores Comparison](#comparison)\n",
    "8. [Visualizing Cross-Validation](#visualization)\n",
    "9. [Best Practices](#bestpractices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Cross-Validation <a id='introduction'></a>\n",
    "\n",
    "**Cross-validation** is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called `k` that refers to the number of groups that a given data sample is to be split into.\n",
    "\n",
    "### Why Cross-Validation?\n",
    "- **Reduces overfitting**: Helps ensure the model generalizes well to unseen data\n",
    "- **Maximizes data usage**: Uses all data for both training and validation\n",
    "- **Provides robust estimates**: Gives a better estimate of model performance than a single train-test split\n",
    "\n",
    "### Common Cross-Validation Methods:\n",
    "- **K-Fold CV**: Dataset is divided into k subsets (folds)\n",
    "- **Stratified K-Fold CV**: Ensures each fold has the same proportion of classes\n",
    "- **Leave-One-Out CV (LOOCV)**: Each sample is used once as test set\n",
    "- **Time Series Split**: For temporal data, maintains chronological order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, load_iris, load_wine\n",
    "from sklearn.model_selection import (\n",
    "    KFold, StratifiedKFold, LeaveOneOut, \n",
    "    TimeSeriesSplit, cross_val_score, cross_validate\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Data Preparation <a id='data'></a>\n",
    "\n",
    "We'll use multiple datasets to demonstrate cross-validation:\n",
    "1. **Iris Dataset**: Classic multiclass classification problem\n",
    "2. **Wine Dataset**: Another multiclass classification problem\n",
    "3. **Synthetic Dataset**: Custom-generated data for specific demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "\n",
    "print(\"Iris Dataset:\")\n",
    "print(f\"  Samples: {X_iris.shape[0]}\")\n",
    "print(f\"  Features: {X_iris.shape[1]}\")\n",
    "print(f\"  Classes: {np.unique(y_iris)}\")\n",
    "print(f\"  Class distribution: {np.bincount(y_iris)}\")\n",
    "print()\n",
    "\n",
    "# Load Wine dataset\n",
    "wine = load_wine()\n",
    "X_wine, y_wine = wine.data, wine.target\n",
    "\n",
    "print(\"Wine Dataset:\")\n",
    "print(f\"  Samples: {X_wine.shape[0]}\")\n",
    "print(f\"  Features: {X_wine.shape[1]}\")\n",
    "print(f\"  Classes: {np.unique(y_wine)}\")\n",
    "print(f\"  Class distribution: {np.bincount(y_wine)}\")\n",
    "print()\n",
    "\n",
    "# Create synthetic dataset with imbalanced classes\n",
    "X_synthetic, y_synthetic = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=3,\n",
    "    weights=[0.6, 0.3, 0.1],  # Imbalanced classes\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Synthetic Dataset (Imbalanced):\")\n",
    "print(f\"  Samples: {X_synthetic.shape[0]}\")\n",
    "print(f\"  Features: {X_synthetic.shape[1]}\")\n",
    "print(f\"  Classes: {np.unique(y_synthetic)}\")\n",
    "print(f\"  Class distribution: {np.bincount(y_synthetic)}\")\n",
    "\n",
    "# Visualize class distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].bar(np.unique(y_iris), np.bincount(y_iris), color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Iris Dataset - Class Distribution')\n",
    "axes[0].set_xlabel('Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "axes[1].bar(np.unique(y_wine), np.bincount(y_wine), color='salmon', edgecolor='black')\n",
    "axes[1].set_title('Wine Dataset - Class Distribution')\n",
    "axes[1].set_xlabel('Class')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "axes[2].bar(np.unique(y_synthetic), np.bincount(y_synthetic), color='lightgreen', edgecolor='black')\n",
    "axes[2].set_title('Synthetic Dataset - Class Distribution (Imbalanced)')\n",
    "axes[2].set_xlabel('Class')\n",
    "axes[2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-Fold Cross-Validation <a id='kfold'></a>\n",
    "\n",
    "**K-Fold CV** divides the dataset into k equally sized folds. The model is trained k times, each time using k-1 folds for training and 1 fold for validation.\n",
    "\n",
    "### How it works:\n",
    "1. Split data into k folds\n",
    "2. For each fold:\n",
    "   - Use that fold as the test set\n",
    "   - Use remaining k-1 folds as training set\n",
    "   - Train model and evaluate\n",
    "3. Average the k results to get overall performance estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a simple model\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Calculate cross-validation scores\n",
    "cv_scores = cross_val_score(model, X_iris, y_iris, cv=kfold, scoring='accuracy')\n",
    "\n",
    "print(\"K-Fold Cross-Validation Results (k=5):\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Scores for each fold: {cv_scores}\")\n",
    "print(f\"Mean accuracy: {cv_scores.mean():.4f}\")\n",
    "print(f\"Standard deviation: {cv_scores.std():.4f}\")\n",
    "print(f\"95% Confidence interval: {cv_scores.mean():.4f} (+/- {1.96 * cv_scores.std():.4f})\")\n",
    "\n",
    "# Visualize the fold splits\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(X_iris)):\n",
    "    # Create arrays to visualize\n",
    "    indices = np.arange(len(X_iris))\n",
    "    mask = np.zeros(len(X_iris))\n",
    "    mask[test_idx] = 1\n",
    "    \n",
    "    ax.scatter(indices[mask == 0], [i] * len(indices[mask == 0]), \n",
    "               c='blue', marker='s', s=10, label='Train' if i == 0 else '')\n",
    "    ax.scatter(indices[mask == 1], [i] * len(indices[mask == 1]), \n",
    "               c='red', marker='s', s=10, label='Test' if i == 0 else '')\n",
    "\n",
    "ax.set_yticks(range(5))\n",
    "ax.set_yticklabels([f'Fold {i+1}' for i in range(5)])\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_title('K-Fold Cross-Validation: Train/Test Splits')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stratified K-Fold Cross-Validation <a id='stratified'></a>\n",
    "\n",
    "**Stratified K-Fold CV** ensures that each fold has approximately the same percentage of samples of each target class as the complete dataset. This is especially important for:\n",
    "- Imbalanced datasets\n",
    "- Classification problems\n",
    "- When you want to ensure each fold is representative of the whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare K-Fold vs Stratified K-Fold on imbalanced data\n",
    "print(\"Comparison on Imbalanced Synthetic Dataset\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Overall class distribution: {np.bincount(y_synthetic)}\")\n",
    "print(f\"Proportions: {np.bincount(y_synthetic) / len(y_synthetic)}\")\n",
    "print()\n",
    "\n",
    "# Regular K-Fold\n",
    "print(\"Regular K-Fold (may not preserve class proportions):\")\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(X_synthetic)):\n",
    "    test_class_dist = np.bincount(y_synthetic[test_idx])\n",
    "    print(f\"  Fold {i+1} test set class distribution: {test_class_dist} \"\n",
    "          f\"(proportions: {test_class_dist / len(test_idx):.3f})\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Stratified K-Fold\n",
    "print(\"Stratified K-Fold (preserves class proportions):\")\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for i, (train_idx, test_idx) in enumerate(stratified_kfold.split(X_synthetic, y_synthetic)):\n",
    "    test_class_dist = np.bincount(y_synthetic[test_idx])\n",
    "    print(f\"  Fold {i+1} test set class distribution: {test_class_dist} \"\n",
    "          f\"(proportions: {test_class_dist / len(test_idx):.3f})\")\n",
    "\n",
    "# Compare performance\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "cv_scores_regular = cross_val_score(model, X_synthetic, y_synthetic, \n",
    "                                    cv=KFold(n_splits=5, shuffle=True, random_state=42), \n",
    "                                    scoring='accuracy')\n",
    "\n",
    "cv_scores_stratified = cross_val_score(model, X_synthetic, y_synthetic, \n",
    "                                       cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), \n",
    "                                       scoring='accuracy')\n",
    "\n",
    "print()\n",
    "print(\"Performance Comparison:\")\n",
    "print(f\"  Regular K-Fold - Mean: {cv_scores_regular.mean():.4f}, Std: {cv_scores_regular.std():.4f}\")\n",
    "print(f\"  Stratified K-Fold - Mean: {cv_scores_stratified.mean():.4f}, Std: {cv_scores_stratified.std():.4f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_pos = np.arange(5)\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x_pos - width/2, cv_scores_regular, width, label='Regular K-Fold', alpha=0.8)\n",
    "ax.bar(x_pos + width/2, cv_scores_stratified, width, label='Stratified K-Fold', alpha=0.8)\n",
    "\n",
    "ax.axhline(y=cv_scores_regular.mean(), color='blue', linestyle='--', alpha=0.5, label='Regular Mean')\n",
    "ax.axhline(y=cv_scores_stratified.mean(), color='orange', linestyle='--', alpha=0.5, label='Stratified Mean')\n",
    "\n",
    "ax.set_xlabel('Fold')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('K-Fold vs Stratified K-Fold Performance Comparison')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([f'Fold {i+1}' for i in range(5)])\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Leave-One-Out Cross-Validation (LOOCV) <a id='loocv'></a>\n",
    "\n",
    "**LOOCV** is an extreme case of K-Fold where k equals the number of samples. Each sample is used once as a single-item test set.\n",
    "\n",
    "### Advantages:\n",
    "- Maximum use of training data (n-1 samples for training)\n",
    "- Deterministic (no randomness in split)\n",
    "- Good for very small datasets\n",
    "\n",
    "### Disadvantages:\n",
    "- Computationally expensive (n iterations)\n",
    "- High variance in performance estimate\n",
    "- Not suitable for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Note: LOOCV can be slow, so we'll use a small subset\n",
    "# Use only first 50 samples of iris dataset\n",
    "X_small = X_iris[:50]\n",
    "y_small = y_iris[:50]\n",
    "\n",
    "print(f\"Running LOOCV on {len(X_small)} samples...\")\n",
    "print(\"This means the model will be trained {0} times!\".format(len(X_small)))\n",
    "print()\n",
    "\n",
    "# Create Leave-One-Out cross-validator\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Simple model for demonstration\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Perform LOOCV\n",
    "cv_scores_loo = cross_val_score(model, X_small, y_small, cv=loo, scoring='accuracy')\n",
    "\n",
    "print(\"Leave-One-Out Cross-Validation Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Number of iterations: {len(cv_scores_loo)}\")\n",
    "print(f\"Accuracy: {cv_scores_loo.mean():.4f}\")\n",
    "print(f\"Number of correct predictions: {cv_scores_loo.sum():.0f} / {len(cv_scores_loo)}\")\n",
    "print()\n",
    "\n",
    "# Compare with 5-fold CV on same data\n",
    "cv_scores_5fold = cross_val_score(model, X_small, y_small, \n",
    "                                  cv=KFold(n_splits=5, shuffle=True, random_state=42), \n",
    "                                  scoring='accuracy')\n",
    "\n",
    "print(\"Comparison with 5-Fold CV on same data:\")\n",
    "print(f\"  LOOCV - Mean: {cv_scores_loo.mean():.4f}, Std: {cv_scores_loo.std():.4f}\")\n",
    "print(f\"  5-Fold - Mean: {cv_scores_5fold.mean():.4f}, Std: {cv_scores_5fold.std():.4f}\")\n",
    "\n",
    "# Visualize individual predictions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# LOOCV results\n",
    "colors = ['green' if score == 1 else 'red' for score in cv_scores_loo]\n",
    "ax1.bar(range(len(cv_scores_loo)), cv_scores_loo, color=colors, alpha=0.7)\n",
    "ax1.set_xlabel('Sample Index')\n",
    "ax1.set_ylabel('Correct Prediction (1=Yes, 0=No)')\n",
    "ax1.set_title(f'LOOCV: Individual Sample Predictions\\nAccuracy: {cv_scores_loo.mean():.2%}')\n",
    "ax1.set_ylim([-0.1, 1.1])\n",
    "\n",
    "# Comparison boxplot\n",
    "ax2.boxplot([cv_scores_5fold, cv_scores_loo], \n",
    "            labels=['5-Fold CV', 'LOOCV'],\n",
    "            patch_artist=True)\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Distribution of Cross-Validation Scores')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Time Series Split <a id='timeseries'></a>\n",
    "\n",
    "**Time Series Split** is designed for temporal data where the order matters. It respects the temporal order and ensures that:\n",
    "- Training data always comes before test data\n",
    "- No future information leaks into the training set\n",
    "\n",
    "This is crucial for time series forecasting, stock prediction, and other temporal problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create synthetic time series data\n",
    "n_samples = 200\n",
    "time = np.arange(n_samples)\n",
    "\n",
    "# Generate trend + seasonality + noise\n",
    "trend = 0.05 * time\n",
    "seasonality = 10 * np.sin(2 * np.pi * time / 50)\n",
    "noise = np.random.normal(0, 2, n_samples)\n",
    "y_timeseries = trend + seasonality + noise\n",
    "\n",
    "# Create features (lagged values)\n",
    "lag_features = 5\n",
    "X_timeseries = np.array([y_timeseries[i:i+lag_features] for i in range(n_samples - lag_features)])\n",
    "y_timeseries = y_timeseries[lag_features:]\n",
    "\n",
    "# Time Series Split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print(\"Time Series Cross-Validation Splits:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualize the splits\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Plot 1: Time series data\n",
    "ax1.plot(y_timeseries, label='Time Series Data', alpha=0.7)\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Value')\n",
    "ax1.set_title('Sample Time Series Data')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Train/Test splits\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X_timeseries)):\n",
    "    print(f\"Fold {i+1}:\")\n",
    "    print(f\"  Train: indices {train_idx[0]} to {train_idx[-1]} (n={len(train_idx)})\")\n",
    "    print(f\"  Test:  indices {test_idx[0]} to {test_idx[-1]} (n={len(test_idx)})\")\n",
    "    print()\n",
    "    \n",
    "    # Visualize splits\n",
    "    indices = np.arange(len(X_timeseries))\n",
    "    \n",
    "    ax2.scatter(train_idx, [i] * len(train_idx), c='blue', marker='s', s=10, \n",
    "                label='Train' if i == 0 else '')\n",
    "    ax2.scatter(test_idx, [i] * len(test_idx), c='red', marker='s', s=10, \n",
    "                label='Test' if i == 0 else '')\n",
    "\n",
    "ax2.set_yticks(range(5))\n",
    "ax2.set_yticklabels([f'Split {i+1}' for i in range(5)])\n",
    "ax2.set_xlabel('Time Index')\n",
    "ax2.set_title('Time Series Cross-Validation: Train/Test Splits\\n(Note: Training set grows, test set moves forward)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Perform cross-validation with a simple model\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "cv_scores_ts = cross_val_score(model, X_timeseries, y_timeseries, \n",
    "                               cv=tscv, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(\"Time Series CV Performance (Ridge Regression):\")\n",
    "print(f\"  Mean Squared Error: {-cv_scores_ts.mean():.4f} (+/- {cv_scores_ts.std():.4f})\")\n",
    "print(f\"  Scores: {-cv_scores_ts}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation Scores Comparison <a id='comparison'></a>\n",
    "\n",
    "Let's compare multiple models using cross-validation to select the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define multiple models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', random_state=42)\n",
    "}\n",
    "\n",
    "# Use stratified k-fold\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "mean_scores = {}\n",
    "std_scores = {}\n",
    "\n",
    "print(\"Model Comparison using 5-Fold Cross-Validation on Wine Dataset\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Create pipeline with scaling (important for SVM and Logistic Regression)\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Perform cross-validation with multiple metrics\n",
    "    cv_results = cross_validate(\n",
    "        pipeline, X_wine, y_wine, cv=cv_strategy,\n",
    "        scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    results[name] = cv_results\n",
    "    mean_scores[name] = cv_results['test_accuracy'].mean()\n",
    "    std_scores[name] = cv_results['test_accuracy'].std()\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Accuracy:  {cv_results['test_accuracy'].mean():.4f} (+/- {cv_results['test_accuracy'].std():.4f})\")\n",
    "    print(f\"  Precision: {cv_results['test_precision_macro'].mean():.4f} (+/- {cv_results['test_precision_macro'].std():.4f})\")\n",
    "    print(f\"  Recall:    {cv_results['test_recall_macro'].mean():.4f} (+/- {cv_results['test_recall_macro'].std():.4f})\")\n",
    "    print(f\"  F1-Score:  {cv_results['test_f1_macro'].mean():.4f} (+/- {cv_results['test_f1_macro'].std():.4f})\")\n",
    "    print(f\"  Train Acc: {cv_results['train_accuracy'].mean():.4f} (+/- {cv_results['train_accuracy'].std():.4f})\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    overfit = cv_results['train_accuracy'].mean() - cv_results['test_accuracy'].mean()\n",
    "    if overfit > 0.1:\n",
    "        print(f\"  ‚ö†Ô∏è  Potential overfitting detected (gap: {overfit:.4f})\")\n",
    "\n",
    "# Find best model\n",
    "best_model = max(mean_scores, key=mean_scores.get)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Best Model: {best_model} with accuracy {mean_scores[best_model]:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "model_names = list(models.keys())\n",
    "accuracies = [results[name]['test_accuracy'] for name in model_names]\n",
    "\n",
    "axes[0, 0].boxplot(accuracies, labels=model_names)\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('Model Accuracy Comparison (5-Fold CV)')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Plot 2: Mean scores with error bars\n",
    "x_pos = np.arange(len(model_names))\n",
    "means = [mean_scores[name] for name in model_names]\n",
    "stds = [std_scores[name] for name in model_names]\n",
    "\n",
    "axes[0, 1].bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 1].set_xticks(x_pos)\n",
    "axes[0, 1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[0, 1].set_ylabel('Mean Accuracy')\n",
    "axes[0, 1].set_title('Mean Accuracy with Standard Deviation')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Train vs Test accuracy (overfitting check)\n",
    "train_means = [results[name]['train_accuracy'].mean() for name in model_names]\n",
    "test_means = [results[name]['test_accuracy'].mean() for name in model_names]\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 0].bar(x_pos - width/2, train_means, width, label='Train', alpha=0.8)\n",
    "axes[1, 0].bar(x_pos + width/2, test_means, width, label='Test', alpha=0.8)\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].set_title('Train vs Test Accuracy (Overfitting Check)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Multiple metrics comparison for best model\n",
    "metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "best_results = results[best_model]\n",
    "metric_scores = [best_results[f'test_{metric}'].mean() for metric in metrics]\n",
    "\n",
    "axes[1, 1].bar(metric_names, metric_scores, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title(f'Multiple Metrics for Best Model: {best_model}')\n",
    "axes[1, 1].set_ylim([0, 1.1])\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(metric_scores):\n",
    "    axes[1, 1].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing Cross-Validation <a id='visualization'></a>\n",
    "\n",
    "Let's create comprehensive visualizations to understand how cross-validation affects model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare different k values for k-fold CV\n",
    "k_values = [2, 3, 5, 10, 15, 20]\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "mean_accuracies = []\n",
    "std_accuracies = []\n",
    "cv_times = []\n",
    "\n",
    "print(\"Effect of Different K Values on Cross-Validation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import time\n",
    "\n",
    "for k in k_values:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    cv_strategy = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_wine, y_wine, cv=cv_strategy, scoring='accuracy')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    mean_accuracies.append(scores.mean())\n",
    "    std_accuracies.append(scores.std())\n",
    "    cv_times.append(elapsed_time)\n",
    "    \n",
    "    print(f\"k={k:2d}: Mean={scores.mean():.4f}, Std={scores.std():.4f}, Time={elapsed_time:.2f}s\")\n",
    "\n",
    "# Visualize the effect of k\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Mean accuracy vs k\n",
    "axes[0].plot(k_values, mean_accuracies, marker='o', linewidth=2, markersize=8)\n",
    "axes[0].fill_between(k_values, \n",
    "                      np.array(mean_accuracies) - np.array(std_accuracies),\n",
    "                      np.array(mean_accuracies) + np.array(std_accuracies),\n",
    "                      alpha=0.3)\n",
    "axes[0].set_xlabel('Number of Folds (k)')\n",
    "axes[0].set_ylabel('Mean Accuracy')\n",
    "axes[0].set_title('Mean Accuracy vs Number of Folds')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Standard deviation vs k\n",
    "axes[1].plot(k_values, std_accuracies, marker='s', color='orange', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Folds (k)')\n",
    "axes[1].set_ylabel('Standard Deviation')\n",
    "axes[1].set_title('Variance in Estimates vs Number of Folds')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Computation time vs k\n",
    "axes[2].plot(k_values, cv_times, marker='^', color='green', linewidth=2, markersize=8)\n",
    "axes[2].set_xlabel('Number of Folds (k)')\n",
    "axes[2].set_ylabel('Computation Time (seconds)')\n",
    "axes[2].set_title('Computational Cost vs Number of Folds')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices <a id='bestpractices'></a>\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Choose the Right CV Method:**\n",
    "   - **K-Fold (k=5 or k=10)**: Default choice for most problems\n",
    "   - **Stratified K-Fold**: Use for classification, especially with imbalanced data\n",
    "   - **LOOCV**: Only for very small datasets (< 100 samples)\n",
    "   - **Time Series Split**: Mandatory for temporal data\n",
    "\n",
    "2. **Typical K Values:**\n",
    "   - k=5: Good balance between bias and variance\n",
    "   - k=10: More reliable but more computationally expensive\n",
    "   - k=n (LOOCV): Maximum bias reduction, but high variance and cost\n",
    "\n",
    "3. **Common Pitfalls:**\n",
    "   - ‚ùå Using regular K-Fold for imbalanced data ‚Üí Use Stratified K-Fold\n",
    "   - ‚ùå Using K-Fold for time series ‚Üí Use Time Series Split\n",
    "   - ‚ùå Data leakage: Scaling/preprocessing before splitting ‚Üí Use pipelines\n",
    "   - ‚ùå Testing on the same data used for hyperparameter tuning ‚Üí Use nested CV\n",
    "\n",
    "4. **Performance Metrics:**\n",
    "   - Always report mean AND standard deviation\n",
    "   - Use multiple metrics (accuracy, precision, recall, F1)\n",
    "   - Check for overfitting (compare train vs. test scores)\n",
    "\n",
    "5. **Computational Considerations:**\n",
    "   - Larger k = more computation but better estimates\n",
    "   - Consider parallel processing for large datasets\n",
    "   - Balance between accuracy of estimate and computational cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example: Proper pipeline to avoid data leakage\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(\"Best Practice Example: Using Pipeline to Prevent Data Leakage\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ‚ùå WRONG WAY - Data leakage\n",
    "print(\"\\n‚ùå INCORRECT: Scaling before CV (causes data leakage):\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_wine)  # Fitting on entire dataset!\n",
    "model = SVC(random_state=42)\n",
    "scores_wrong = cross_val_score(model, X_scaled, y_wine, cv=5)\n",
    "print(f\"   Mean accuracy: {scores_wrong.mean():.4f} (INFLATED DUE TO LEAKAGE!)\")\n",
    "\n",
    "# ‚úÖ CORRECT WAY - Use pipeline\n",
    "print(\"\\n‚úÖ CORRECT: Using pipeline (no data leakage):\")\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scaling happens within each fold\n",
    "    ('pca', PCA(n_components=10)),\n",
    "    ('classifier', SVC(random_state=42))\n",
    "])\n",
    "scores_correct = cross_val_score(pipeline, X_wine, y_wine, cv=5)\n",
    "print(f\"   Mean accuracy: {scores_correct.mean():.4f} (CORRECT ESTIMATE)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Difference: {(scores_wrong.mean() - scores_correct.mean()):.4f}\")\n",
    "print(\"   This shows how data leakage can give overly optimistic results!\")\n",
    "\n",
    "# Nested CV example for hyperparameter tuning\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Nested Cross-Validation for Hyperparameter Tuning\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__gamma': ['scale', 'auto'],\n",
    "}\n",
    "\n",
    "# Inner CV for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "clf = GridSearchCV(pipeline, param_grid, cv=inner_cv, scoring='accuracy')\n",
    "\n",
    "# Outer CV for performance estimation\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "nested_scores = cross_val_score(clf, X_wine, y_wine, cv=outer_cv)\n",
    "\n",
    "print(f\"\\nNested CV Results:\")\n",
    "print(f\"  Mean accuracy: {nested_scores.mean():.4f} (+/- {nested_scores.std():.4f})\")\n",
    "print(f\"  Individual fold scores: {nested_scores}\")\n",
    "print(f\"\\n  This is an UNBIASED estimate of model performance with optimized hyperparameters.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered:\n",
    "\n",
    "‚úÖ **What is cross-validation** and why it's important  \n",
    "‚úÖ **K-Fold CV**: The standard approach for most problems  \n",
    "‚úÖ **Stratified K-Fold CV**: Essential for imbalanced classification  \n",
    "‚úÖ **Leave-One-Out CV**: For small datasets only  \n",
    "‚úÖ **Time Series Split**: For temporal data  \n",
    "‚úÖ **Model comparison** using cross-validation  \n",
    "‚úÖ **Visualizations** to understand CV behavior  \n",
    "‚úÖ **Best practices** to avoid common pitfalls  \n",
    "\n",
    "### Next Steps:\n",
    "- Try cross-validation on your own datasets\n",
    "- Experiment with different k values\n",
    "- Implement nested CV for hyperparameter tuning\n",
    "- Explore other CV strategies (ShuffleSplit, GroupKFold, etc.)\n",
    "\n",
    "Happy learning! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
